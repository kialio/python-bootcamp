{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<CENTER>\n",
      "<H1>\n",
      "NASA Goddard Space Flight Center <BR>\n",
      "    Python User Group <BR>\n",
      "2014 Python Boot Camp\n",
      "</H1>\n",
      "</CENTER>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scientific Programming II: Machine Learning"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Reference Document"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<OL>\n",
      "<LI> <A HREF=\"http://scikit-learn.org/stable/documentation.html\">scikit-learn Documentation</A>\n",
      "</OL>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib\n",
      "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi'] # Increase default figure resolution"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Machine Learning: Problem Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In general, a learning problem considers a set of n samples of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), is it said to have several attributes or features.\n",
      "\n",
      "We can separate learning problems in a few large categories:\n",
      "\n",
      "- Supervised learning, in which the data comes with additional attributes that we want to predict (Click here to go to the scikit-learn supervised learning page).This problem can be either:\n",
      "\n",
      "    - Classification: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data. An example of classification problem would be the handwritten digit recognition example, in which the aim is to assign each input vector to one of a finite number of discrete categories. Another way to think of classification is as a discrete (as opposed to continuous) form of supervised learning where one has a limited number of categories and for each of the n samples provided, one is to try to label them with the correct category or class.\n",
      "    - Regression: if the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight.\n",
      "\n",
      "- Unsupervised learning, in which the training data consists of a set of input vectors x without any corresponding target values. The goal in such problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization (Click here to go to the Scikit-Learn unsupervised learning page).\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Iris Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider three related species of the Iris flower: setosa, virginica, and versicolor"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg\" width=400> Iris setosa <br>\n",
      "<img src=\"http://upload.wikimedia.org/wikipedia/commons/9/9f/Iris_virginica.jpg\" width=400> Iris virginica <br>\n",
      "<img src=\"http://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg\" width=400> Iris versicolor <br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Imagine you have a database of measurements of the lengths and widths of the sepal (flowering part) and the petal for 50 flowers of each species (i.e., 150 in total).  You find an iris growing in your local park and want to know what species it belongs to.  Being a diligent scientist, you measure the sepal and petal length and width. How can you determine the species of this flower?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Classifying Iris Species"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "iris = datasets.load_iris()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print iris.DESCR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The \"features\" are stored in the data attribute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dat = iris.data\n",
      "dat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While the \"classifications\" are stored in the target attribute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl = iris.target\n",
      "cl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For classification problems (i.e., discretized), the target labels can be found in the \"target_names\" attribute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris.target_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are *many* different kinds of machine learning estimators, and identifying the proper algorithm for the task at hand is often the most difficult part of the job.  For classification tasks with only a modest amount of data, Support Vector Machines (SVMs) are usually a good place to start. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "clf = svm.SVC(kernel=\"linear\", probability=True) # Enables probability estimates (see below)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we've created a new instance of a Support Vector classifier.  To simplify our lives, we'll only consider petal length and width (this makes the results easier to visualize).  To perform the classification fit, run:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = dat[:,2:]; Y = cl\n",
      "clf.fit(X, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Essentially this divides the \"phase space\" up into chunks based on the location of known objects."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
      "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
      "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "\n",
      "# Put the result into a color plot\n",
      "Z = Z.reshape(xx.shape)\n",
      "plt.figure()\n",
      "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
      "\n",
      "# Plot also the training points\n",
      "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
      "plt.xlabel('Petal length')\n",
      "plt.ylabel('Petal width')\n",
      "\n",
      "plt.xlim(xx.min(), xx.max())\n",
      "plt.ylim(yy.min(), yy.max())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can then use this to \"classify\" the object(s) we found in our local park.  Suppose they had the following petal length and widths:\n",
      "\n",
      "1.25  0.34 <br>\n",
      "1.45  0.27 <br>\n",
      "1.83  0.65"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.predict([[1.25, 0.34], [1.45, 0.27], [1.83, 0.65]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These all appear to belong to the setosa species, as can be seen from adding them to the plot above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure()\n",
      "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
      "\n",
      "# Plot also the training points\n",
      "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
      "plt.xlabel('Petal length')\n",
      "plt.ylabel('Petal width')\n",
      "\n",
      "plt.xlim(xx.min(), xx.max())\n",
      "plt.ylim(yy.min(), yy.max())\n",
      "\n",
      "plt.plot([1.25, 1.45, 1.83], [0.34, 0.27, 0.65], \"ks\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is also possible to return probabilistic estimates of the classifications (instead of just the most likely one).  For SV estimators, these are not always reliable (though in this case the classification is pretty obvious)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.predict_proba([[1.25, 0.34], [1.45, 0.27], [1.83, 0.65]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The shape of the dividing lines between classes depend on the kernel used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf2 = svm.SVC(kernel=\"poly\", degree=3, probability=True)\n",
      "clf2.fit(X, Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z2 = clf2.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "\n",
      "# Put the result into a color plot\n",
      "Z2 = Z2.reshape(xx.shape)\n",
      "plt.figure()\n",
      "plt.pcolormesh(xx, yy, Z2, cmap=plt.cm.Paired)\n",
      "\n",
      "# Plot also the training points\n",
      "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
      "plt.xlabel('Petal length')\n",
      "plt.ylabel('Petal width')\n",
      "\n",
      "plt.xlim(xx.min(), xx.max())\n",
      "plt.ylim(yy.min(), yy.max())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Facial Recognition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Facial recognition is a common task for machine learning algorithms.  We'll explore here the decomposition into \"eigenfaces\", as well as concepts like \"cross-validation\".  We'll use the \u201cLabeled Faces in the Wild\u201d data set, also known as LFW.\n",
      "\n",
      "First, we need to fetch some images of well-known people.  Note that this may take a while (depending on bandwidth)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_lfw_people\n",
      "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples, h, w = lfw_people.images.shape\n",
      "print n_samples, h, w\n",
      "lfw_people.target_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have 1288 pictures of 7 people.  Each picture has 50 x 37 (1850) pixels.  Here's an example photo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matplotlib.rcParams['savefig.dpi'] = matplotlib.rcParams['savefig.dpi'] / 2 # Decrease default figure resolution\n",
      "plt.imshow(lfw_people.data[0].reshape(h, w), cmap=cm.gray)\n",
      "plt.xticks(()); plt.yticks(());"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Split up the data and classifications."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = lfw_people.data; Y = lfw_people.target;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now split everything up into two groups: the training set and the testing set.  If done properly, this allows us to \"cross-validate\" our results.  In this case, we'll reserve 25% of the images for the validation (test) set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use the training set to perform a Principle Component Analysis, breaking down each image into a (fixed) number of \"eigenfaces\".  When performing PCA, often the most difficult task is to determine how many eigenspectra to use.  Typically you will need to perform the analysis with many different values, and see where the improvement in cross-validation \"flattens\" out.  For simplicity here we will just arbitrarily choose 150 components."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import RandomizedPCA\n",
      "n_components = 150\n",
      "pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want to project all the faces onto this orthonormal basis (this should ideally save lots of time when performing the estimator fit)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_pca = pca.transform(X_train)\n",
      "X_test_pca = pca.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once again we will use an SVM model for our algorithm (a slightly different implementation).  But we will conduct a grid search to speed things up a bit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5], 'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
      "clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)\n",
      "clf = clf.fit(X_train_pca, Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we want to use the test set to see how well we did (cross-validation)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_pred = clf.predict(X_test_pca)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are nice convenient ways to display the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "print(classification_report(Y_test, Y_pred, target_names=lfw_people.target_names))\n",
      "print(confusion_matrix(Y_test, Y_pred, labels=range(len(lfw_people.target_names))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's plot a few examples to see how we did."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_gallery(images, titles, h, w, n_row=2, n_col=8):\n",
      "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
      "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
      "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
      "    for i in range(n_row * n_col):\n",
      "        plt.subplot(n_row, n_col, i + 1)\n",
      "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
      "        plt.title(titles[i], size=12)\n",
      "        plt.xticks(())\n",
      "        plt.yticks(())\n",
      "\n",
      "\n",
      "# plot the result of the prediction on a portion of the test set\n",
      "\n",
      "def title(y_pred, y_test, target_names, i):\n",
      "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
      "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
      "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
      "\n",
      "prediction_titles = [title(Y_pred, Y_test, lfw_people.target_names, i)\n",
      "                     for i in range(Y_pred.shape[0])]\n",
      "\n",
      "plot_gallery(X_test, prediction_titles, h, w)\n",
      "\n",
      "# plot the gallery of the most significative eigenfaces\n",
      "\n",
      "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
      "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
      "plot_gallery(eigenfaces, eigenface_titles, h, w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}